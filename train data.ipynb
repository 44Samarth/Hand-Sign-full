{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classiffier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads hand gesture data from a pickle file.\n",
    "- Splits the data into training and testing sets.\n",
    "- Trains a Support Vector Classifier (SVC) model using the training data.\n",
    "- Evaluates the model's accuracy on the testing data.\n",
    "- Saves the trained model to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading Data: The script first loads the hand gesture data from a pickle file named 'data.pickle'. This file contains the preprocessed data, including the hand landmarks extracted from the images and their corresponding labels.\n",
    "\n",
    "* Splitting Data: After loading the data, it splits it into two sets: training data and testing data. The train_test_split function from scikit-learn is used for this purpose. By default, it splits the data into 80% training and 20% testing sets.\n",
    "\n",
    "* Model Initialization: Next, it initializes a Support Vector Classifier (SVC) model. SVC is a popular supervised learning algorithm used for classification tasks. In this case, it's chosen for its effectiveness in handling high-dimensional data like the hand landmarks extracted from images.\n",
    "\n",
    "* Model Training: The initialized SVC model is trained using the training data. The fit method is called on the model object (model.fit(x_train, y_train)), where x_train represents the features (hand landmarks) and y_train represents the corresponding labels.\n",
    "\n",
    "* Model Evaluation: Once the model is trained, it predicts the labels for the testing data using the predict method (y_predict = model.predict(x_test)). Then, it calculates the accuracy of the model by comparing the predicted labels (y_predict) with the actual labels (y_test). The accuracy_score function from scikit-learn is used for this purpose.\n",
    "\n",
    "* Saving the Model: Finally, the trained model is saved to a file named 'model.p' using pickle. This allows you to reuse the trained model later without needing to retrain it every time.\n",
    "\n",
    "* Print Results: The script prints the accuracy of the model on the testing data, indicating how well the model performs in classifying hand gestures. It also prints the percentage of samples that were classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier # For ensemble\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open('./data.pickle', 'rb')) # read the data from pickle file\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "x_train,x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, shuffle=True, stratify=labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_predict, average='weighted')\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_predict, average='weighted')\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_predict, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "f = open('model.p', 'wb') # write the model to a file\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Used in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: The overall accuracy of the model is very high, approximately 99.62%. This indicates that the model correctly classified the majority of the hand gestures in the testing dataset.\n",
    "\n",
    "- Precision: The precision score measures the proportion of correctly predicted positive samples out of all samples predicted as positive. The precision score for each class is very high, indicating that the model has a low rate of false positives across all classes.\n",
    "\n",
    "- Recall: The recall score measures the proportion of correctly predicted positive samples out of all actual positive samples. The recall score for each class is very high, indicating that the model effectively captures most of the positive samples for each class.\n",
    "\n",
    "- F1-score: The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. The F1-score for each class is very high, indicating that the model achieves both high precision and high recall for each class.\n",
    "\n",
    "- Support: The support column indicates the number of samples for each class in the testing dataset. It helps to understand the distribution of samples across different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### sklearn\n",
    "- User-Friendly Interface: scikit-learn provides a consistent and easy-to-use interface for various machine learning tasks, including classification, regression, clustering, dimensionality reduction, and more.\n",
    "\n",
    "- Wide Range of Algorithms: It implements a wide range of machine learning algorithms, including but not limited to:\n",
    "\n",
    "  - Supervised learning algorithms like Support Vector Machines (SVM), Decision Trees, Random Forests, Gradient Boosting, k-Nearest Neighbors (k-NN), and Neural Networks.\n",
    "  - Unsupervised learning algorithms like K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), and Independent Component Analysis (ICA).\n",
    "\n",
    "- Model Evaluation and Validation: scikit-learn provides tools for model evaluation, including metrics such as accuracy, precision, recall, F1-score, ROC curves, and more. It also offers functions for cross-validation and hyperparameter tuning to improve model performance.\n",
    "\n",
    "- Data Preprocessing: The library includes various utilities for data preprocessing, such as feature scaling, feature selection, data imputation, encoding categorical variables, and handling missing values.\n",
    "\n",
    "- Integration with Other Libraries: scikit-learn seamlessly integrates with other popular Python libraries like NumPy, SciPy, Pandas, and Matplotlib, making it easy to incorporate machine learning into data analysis workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all model at once and print there metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def train_model_and_evaluate(model, data, labels, test_size=0.2, random_state=42):\n",
    "    # Split the data into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, random_state=random_state, shuffle=True, stratify=labels)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing data\n",
    "    y_predict = model.predict(x_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    precision = precision_score(y_test, y_predict, average='weighted')\n",
    "    recall = recall_score(y_test, y_predict, average='weighted')\n",
    "    f1 = f1_score(y_test, y_predict, average='weighted')\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_predict)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, report\n",
    "\n",
    "# Load data from pickle file\n",
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "# Define models to train\n",
    "models = [\n",
    "    (\"Support_Vector_Classifier_(SVC)\", SVC(kernel='linear')),\n",
    "    (\"Random_Forest_Classifier\", RandomForestClassifier()),\n",
    "    (\"Logistic_Regression\", LogisticRegression()),\n",
    "    (\"K-Nearest_Neighbors_(KNN)\", KNeighborsClassifier()),\n",
    "    (\"Gradient_Boosting_Classifier\", GradientBoostingClassifier())\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    accuracy, precision, recall, f1, report = train_model_and_evaluate(model, data, labels)\n",
    "    \n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "\n",
    "    # Save the trained model to a file\n",
    "    os.makedirs('./all_models/', exist_ok=True)\n",
    "    with open(f'./all_models/{model_name}_model.p', 'wb') as f:\n",
    "        pickle.dump({'model': model}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Random Forest is a versatile machine learning algorithm commonly used for classification and regression tasks. It belongs to the family of ensemble learning methods, which combine multiple individual models (often referred to as \"weak learners\") to form a stronger, more robust model. \n",
    "\n",
    "- Random Forest algorithm: Ensemble Learning: Random Forest is an ensemble learning technique that combines multiple decision trees to make predictions. It builds multiple decision trees during training and merges their predictions to obtain a final prediction. This process is known as bagging (Bootstrap Aggregating).\n",
    "\n",
    "- Decision Trees: Each decision tree in the Random Forest is trained independently on a random subset of the training data and a random subset of features. This randomness helps to decorrelate the trees and reduces overfitting.\n",
    "\n",
    "- Random Feature Selection: At each node of the decision tree, a random subset of features is considered for splitting. This randomness introduces diversity among the trees, making the ensemble more robust.\n",
    "\n",
    "- Voting or Averaging: For classification tasks, the Random Forest combines the predictions of individual trees by majority voting. For regression tasks, it combines the predictions by averaging.\n",
    "\n",
    "- Feature Importance: Random Forest can provide information about the importance of features in making predictions. It calculates feature importance based on how much the tree nodes that use the feature reduce impurity across all the trees in the forest.\n",
    "\n",
    "- Robustness: Random Forest is less prone to overfitting compared to individual decision trees, especially when trained with a large number of trees. It typically performs well on various types of data without much hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try this for Random Forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the trained model from the 'model.p' file\n",
    "with open('./all_models/Random Forest Classifier_model.p', 'rb') as f:\n",
    "    model_data = pickle.load(f)\n",
    "\n",
    "# Extract the trained model object from the loaded data\n",
    "model = model_data['model']\n",
    "\n",
    "# Now you can access various properties and methods of the model to get a summary\n",
    "# For example, you can print the parameters used to train the model\n",
    "print(\"Model Parameters:\")\n",
    "print(model.get_params())\n",
    "\n",
    "# You can also print the feature importances if available\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(model.feature_importances_)\n",
    "\n",
    "# Additionally, you can print other relevant information about the model\n",
    "print(\"\\nModel Summary:\")\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 42 Important features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Model Parameters:**\n",
    "   - `bootstrap`: Whether bootstrap samples are used when building trees. In this case, it's set to `True`, meaning that bootstrap samples are used.\n",
    "   - `ccp_alpha`: Complexity parameter used for Minimal Cost-Complexity Pruning. It's set to `0.0`, meaning no pruning is applied.\n",
    "   - `class_weight`: Weights associated with classes. It's set to `None`, indicating that all classes are treated equally.\n",
    "   - `criterion`: The function to measure the quality of a split. Here, it's set to 'gini', which measures the impurity of the nodes.\n",
    "   - `max_depth`: The maximum depth of the tree. It's set to `None`, allowing nodes to expand until all leaves are pure or until they contain `min_samples_split` samples.\n",
    "   - `max_features`: The number of features to consider when looking for the best split. It's set to 'sqrt', meaning the square root of the number of features is considered.\n",
    "   - `min_samples_leaf`: The minimum number of samples required to be at a leaf node. It's set to `1`.\n",
    "   - `n_estimators`: The number of trees in the forest. It's set to `100`.\n",
    "   - Other parameters control various aspects of the model, such as the splitting strategy, leaf node criteria, and more.\n",
    "\n",
    "2. **Feature Importances:**\n",
    "   - These values represent the importance of each feature in making predictions. Features with higher importances are more influential in the model's decision-making process.\n",
    "   - The feature importances are provided as an array, with each value corresponding to a feature. The higher the value, the more important the feature.\n",
    "\n",
    "3. **Model Summary:**\n",
    "   - This line simply indicates the type of model used, which is `RandomForestClassifier()`.\n",
    "\n",
    "Based on this summary:\n",
    "- The Random Forest classifier uses default parameters with no specific adjustments.\n",
    "- The model has been trained using 100 decision trees (`n_estimators=100`).\n",
    "- Feature importances are available, indicating the relative importance of each feature in making predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating decision tree made during the fitting process for Random forest classifier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here we can manually give tree index for its visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.estimators_` is an attribute of a scikit-learn ensemble model (such as `RandomForestClassifier` or `GradientBoostingClassifier`) that contains a list of individual decision tree estimators in the ensemble.\n",
    "\n",
    "In the case of a `RandomForestClassifier`, which is an ensemble of decision trees, `model.estimators_` provides access to each individual decision tree in the random forest. Each decision tree in the list represents one of the trees trained during the random forest fitting process.\n",
    "\n",
    "we can use `model.estimators_` to access and manipulate each individual decision tree, such as visualizing them separately, as shown in the previous code snippet. This attribute is useful for understanding the internal structure of the ensemble model and analyzing the contributions of individual trees to the overall predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained RandomForestClassifier model from model.p\n",
    "with open('model.p', 'rb') as f:\n",
    "    model = pickle.load(f)['model']\n",
    "\n",
    "# Define the index of the tree you want to visualize\n",
    "tree_index =0  # Change this to the index of the tree you want to visualize (0 to n_trees-1)\n",
    "\n",
    "# Check if the specified index is within the range of the number of trees in the forest\n",
    "if tree_index < len(model.estimators_):\n",
    "    # Visualize the specified tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(model.estimators_[tree_index], filled=True)\n",
    "    plt.title(f'Tree {tree_index + 1}')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Error: Tree index {tree_index} is out of range. There are only {len(model.estimators_)} trees in the forest.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained RandomForestClassifier model from model.p\n",
    "with open('model.p', 'rb') as f:\n",
    "    model = pickle.load(f)['model']\n",
    "\n",
    "# Define the index of the tree you want to visualize\n",
    "tree_index = 0  # Change this to the index of the tree you want to visualize\n",
    "\n",
    "# Check if the specified index is within the range of the number of trees in the forest\n",
    "if tree_index < len(model.estimators_):\n",
    "    # Visualize the specified tree with improved image quality\n",
    "    plt.figure(figsize=(50, 50))\n",
    "    plot_tree(model.estimators_[tree_index], \n",
    "              filled=True, \n",
    "              feature_names=None,  # Set to None to use default feature names   \n",
    "              class_names=None,    # Set to None to use default class names\n",
    "              fontsize=8)          # Adjust fontsize to make text more visible\n",
    "    plt.title(f'Tree {tree_index + 1}')\n",
    "    # make dirs for images\n",
    "    os.makedirs('./all_trees/', exist_ok=True)\n",
    "    plt.savefig(f'./all_trees/tree_{tree_index + 1}_visualization.png', dpi=300)  # Save the plot as high-quality PNG\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Error: Tree index {tree_index} is out of range. There are only {len(model.estimators_)} trees in the forest.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision-making process of a Random Forest classifier involves several steps internally:\n",
    "\n",
    "1. **Feature Selection:**\n",
    "   - Random Forest randomly selects a subset of features from the input dataset for each tree. This process is known as feature selection or feature bagging.\n",
    "   - The number of features considered at each split point is determined by the `max_features` parameter.\n",
    "\n",
    "2. **Building Decision Trees:**\n",
    "   - For each tree in the forest, a bootstrap sample (randomly sampled dataset with replacement) is created from the original dataset.\n",
    "   - Using the bootstrap sample, a decision tree is constructed. The tree is grown recursively by selecting the best split at each node based on a criterion, usually the Gini impurity or entropy.\n",
    "   - The `max_depth`, `min_samples_split`, and `min_samples_leaf` parameters control the growth of each individual tree to prevent overfitting.\n",
    "\n",
    "3. **Voting Mechanism:**\n",
    "   - During prediction, each tree in the forest independently predicts the class of the input sample.\n",
    "   - For classification tasks, the mode (most frequent class) of the predictions from all the trees is taken as the final prediction.\n",
    "   - For regression tasks, the average of the predictions from all the trees is taken as the final prediction.\n",
    "\n",
    "4. **Bootstrap Aggregating (Bagging):**\n",
    "   - Random Forest employs an ensemble learning technique called bagging (Bootstrap Aggregating).\n",
    "   - It creates multiple decision trees trained on different bootstrap samples of the training data.\n",
    "   - Bagging helps in reducing overfitting and improving the stability and accuracy of the model.\n",
    "\n",
    "5. **Out-of-Bag (OOB) Evaluation:**\n",
    "   - Random Forest uses out-of-bag samples to estimate the generalization performance of the model.\n",
    "   - Out-of-bag samples are the data points that are not included in the bootstrap sample used to train each tree.\n",
    "   - These samples are used to evaluate the performance of each tree individually without the need for a separate validation set.\n",
    "\n",
    "Overall, the Random Forest classifier combines the predictions of multiple decision trees to improve the robustness and accuracy of the model, making it a powerful tool for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This below code is computationally expensive it creates 100 trees from the model, Dont run unless its needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import plot_tree\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the trained RandomForestClassifier model from model.p\n",
    "# with open('model.p', 'rb') as f:\n",
    "#     model = pickle.load(f)['model']\n",
    "\n",
    "# # Visualize each tree in the random forest ensemble\n",
    "# for i, estimator in enumerate(model.estimators_):\n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#     plot_tree(estimator[0], filled=True)\n",
    "#     plt.title(f'Tree {i+1}')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Handfeb17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
