{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requiremnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install mediapipe==0.9.0.1 scikit-learn==1.2.0 opencv-python==4.7.0.68\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_of_classes = 26  # Assuming you have 26 classes\n",
    "dataset_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "number_of_classes = 26  # Assuming you have 26 classes\n",
    "dataset_size = 100\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        exit()\n",
    "\n",
    "    for j in range(number_of_classes):\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, str(j))):\n",
    "            os.makedirs(os.path.join(DATA_DIR, str(j)))\n",
    "\n",
    "        print('Collecting data for class {}'.format(j))\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            cv2.putText(frame, 'Ready? Press \"S\" to start capturing, \"N\" to skip, or \"q\" to exit!', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "            cv2.imshow('frame', frame)\n",
    "            key = cv2.waitKey(25)\n",
    "            \n",
    "            if key == ord('s'):\n",
    "                print(\"Capturing data for class {}\".format(j))\n",
    "                break\n",
    "            elif key == ord('n'):\n",
    "                print(\"Skipping class {}\".format(j))\n",
    "                break\n",
    "            elif key == ord('q'):\n",
    "                print(\"Exiting the program.\")\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "\n",
    "        if key == ord('n'):\n",
    "            continue  # Skip to the next class\n",
    "\n",
    "        counter = 0\n",
    "        while counter < dataset_size:\n",
    "            ret, frame = cap.read()\n",
    "            cv2.imshow('frame', frame)\n",
    "            cv2.waitKey(25)\n",
    "            cv2.imwrite(os.path.join(DATA_DIR, str(j), '{}.jpg'.format(counter)), frame)\n",
    "            counter += 1\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # To show the land marks on the image (only last frame \"Just for Understanding\")\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "DATA_DIR = './data'\n",
    "\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_))[:1]:\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hands_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(img_rgb, # image to draw \n",
    "                                          hands_landmarks, # Model output\n",
    "                                          mp_hands.HAND_CONNECTIONS, # hand connections\n",
    "                                          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                                          mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "           \n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(img_rgb)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'\n",
    "\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []\n",
    "\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)\n",
    "\n",
    "f = open('data.pickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mediapipe, like many other image processing libraries and models, expects input images to be in RGB format. RGB (Red, Green, Blue) is a standard color space used in computer vision and image processing tasks. In RGB format, the color channels are arranged as red, green, and blue, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization: The extracted x and y coordinates are normalized by subtracting the minimum x and y values from all coordinates. Normalization helps in making the data scale-invariant and reduces the impact of varying hand sizes and positions in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classiffier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads hand gesture data from a pickle file.\n",
    "- Splits the data into training and testing sets.\n",
    "- Trains a Support Vector Classifier (SVC) model using the training data.\n",
    "- Evaluates the model's accuracy on the testing data.\n",
    "- Saves the trained model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  # For SVC\n",
    "from sklearn.ensemble import RandomForestClassifier # For ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open('./data.pickle', 'rb')) # read the data from pickle file\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "x_train,x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, shuffle=True, stratify=labels)\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "model = SVC(kernel='linear')  # You can choose different kernels like 'rbf' or 'poly' as well\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "print('{}% of samples were classified correctly !'.format(score * 100))\n",
    "\n",
    "f = open('model.p', 'wb') # write the model to a file\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "# labels from 0 to 12\n",
    "labels_dict = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I', 9:'J', 10:'K', 11:'L', 12:'M', 13:'N', 14:'O', 15:'P', 16:'Q', 17:'R', 18:'S', 19:'T', 20:'U', 21:'V', 22:'W', 23:'X', 24:'Y', 25:'Z'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# while True:\n",
    "\n",
    "#     data_aux = []\n",
    "#     x_ = []\n",
    "#     y_ = []\n",
    "\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     H, W, _ = frame.shape\n",
    "\n",
    "#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     results = hands.process(frame_rgb)\n",
    "#     if results.multi_hand_landmarks:\n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 frame,  # image to draw\n",
    "#                 hand_landmarks,  # model output\n",
    "#                 mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "#                 mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "#                 mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             for i in range(len(hand_landmarks.landmark)):\n",
    "#                 x = hand_landmarks.landmark[i].x\n",
    "#                 y = hand_landmarks.landmark[i].y\n",
    "\n",
    "#                 x_.append(x)\n",
    "#                 y_.append(y)\n",
    "\n",
    "#             for i in range(len(hand_landmarks.landmark)):\n",
    "#                 x = hand_landmarks.landmark[i].x\n",
    "#                 y = hand_landmarks.landmark[i].y\n",
    "#                 data_aux.append(x - min(x_))\n",
    "#                 data_aux.append(y - min(y_))\n",
    "\n",
    "#         x1 = int(min(x_) * W) - 10\n",
    "#         y1 = int(min(y_) * H) - 10\n",
    "\n",
    "#         x2 = int(max(x_) * W) - 10\n",
    "#         y2 = int(max(y_) * H) - 10\n",
    "\n",
    "\n",
    "#         prediction = model.predict([np.asarray(data_aux)])\n",
    "#         # If we not add this condition, the model will try to predict the label even if there is no hand in the frame (that means program will try to find the key even when it is not in the dictionary, which will cause an error)\n",
    "#         if prediction.shape[0] > 0:\n",
    "#             predicted_label_index = int(prediction[0])\n",
    "#             predicted_character = labels_dict.get(predicted_label_index, \"Unknown\")\n",
    "#         else:\n",
    "#             predicted_character = \"Unknown\"\n",
    "#         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "#         cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     c = cv2.waitKey(1)\n",
    "#     if c == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error handling using try-catch (especially multihand error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Data and variables initialization\n",
    "        data_aux = []\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Get the dimensions of the frame\n",
    "        H, W, _ = frame.shape\n",
    "\n",
    "        # Convert the frame to RGB color space\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect hand landmarks\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # If hand landmarks are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,  # image to draw\n",
    "                    hand_landmarks,  # model output\n",
    "                    mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "            x1 = int(min(x_) * W) - 10\n",
    "            y1 = int(min(y_) * H) - 10\n",
    "\n",
    "            x2 = int(max(x_) * W) - 10\n",
    "            y2 = int(max(y_) * H) - 10\n",
    "\n",
    "            # Make predictions using the model\n",
    "            prediction = model.predict([np.asarray(data_aux)])\n",
    "\n",
    "            # If predictions are available\n",
    "            if prediction.shape[0] > 0:\n",
    "                predicted_label_index = int(prediction[0])\n",
    "                predicted_character = labels_dict.get(predicted_label_index, \"Unknown\")\n",
    "            else:\n",
    "                predicted_character = \"Unknown\"\n",
    "\n",
    "            # Draw bounding box and label on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "            cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        # Check for user input to exit the loop\n",
    "        c = cv2.waitKey(1)\n",
    "        if c == ord('q'):\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print the exception\n",
    "        print(\"An error occurred:\", str(e))\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
